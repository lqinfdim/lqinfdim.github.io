<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Reasoning Language Model Inference Serving Unveiled: An Empirical Study">
  <meta name="keywords" content="RLLM-Serving">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Reasoning Language Model Inference Serving Unveiled: An Empirical Study</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">
  <!-- <link rel="icon" type="image/png" href="figures/logo.png"> -->

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">

  <style>
    .section-title-bar-wrapper {
      background-color: #f5f5f5;
      max-width: 1200px;
      margin: 0 auto 20px auto;
      padding: 20px 0;
      border-radius: 5px;
    }
    .section-title-bar-wrapper .title {
      margin-bottom: 0 !important;
    }
    .subsection-title {
      font-size: 1.75rem;
      font-weight: 600;
      color: #363636;
      margin-bottom: 10px;
    }
    .result-card {
      background-color: #ffffff;
      border-radius: 10px;
      box-shadow: 0 4px 15px rgba(0, 0, 0, 0.1), 0 2px 6px rgba(0, 0, 0, 0.08);
      padding: 25px;
      margin-bottom: 30px;
      transition: box-shadow 0.3s ease, transform 0.3s ease;
    }
    .result-card:hover {
      box-shadow: 0 8px 25px rgba(0, 0, 0, 0.15), 0 4px 10px rgba(0, 0, 0, 0.1);
      transform: translateY(-2px);
    }
  </style>

  <!--  -->
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  
</head>
<body>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title is-bold">
            <img src="figures/logo.png" alt="HERMES logo" style="height: 1.2em; vertical-align: middle; margin-right: 0.1em;">
            <span class="rllm-serving" style="vertical-align: middle">RLLM Serving</span>
            </h1>
          <h2 class="subtitle is-3 publication-subtitle">
            Reasoning Language Model Inference Serving Unveiled: <br> An Empirical Study
          </h2>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://lqinfdim.github.io/">Qi Li</a><sup style="color:#0e419c;">1,2*</sup>,
            </span>
            <span class="author-block">
              <a href="/">Junpan Wu</a><sup style="color:#6fbf73">4*</sup>,
            </span>
            <span class="author-block">
              <a href="/">Xiang Liu</a><sup style="color:#0e419c">2*</sup>,
            </span>
            <span class="author-block">
              <a href="/">Yuxin Wang</a><sup style="color:#ed4b82">3</sup>,
            </span>
            <span class="author-block">
              <a href="/">Zeyu Li</a><sup style="color:#ed4b82">2</sup>,
            </span>
            <span class="author-block">
              <a href="/">Yuhan Chen</a><sup style="color:#ed4b82">2</sup>,
            </span>
            <span class="author-block">
              <a href="/">Shaohuai Shi</a><sup style="color:#ed4b82">5</sup>, <br>
            </span>
            <span class="author-block">
              <a href="/">Zhenheng Tang</a><sup style="color:#0e419c">6</sup><sup style="color:#6fbf73">&dagger;</sup>
            </span>
            <span class="author-block">
              <a href="/">Xiaowen Chu</a><sup style="color:#0e419c">2</sup><sup style="color:#6fbf73">&dagger;</sup>
            </span>
            
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup style="color:#0e419c">1</sup>Tsinghua University,</span>
            <span class="author-block"><sup style="color:#6fbf73;">2</sup>The Hong Kong University of Science and Technology (Guang Zhou),</span><br>
            <span class="author-block"><sup style="color:#ed4b82">3</sup>HKBU, </span>
            <span class="author-block"><sup style="color:#ed4b82">4</sup>University of Wisconsin-Madison,</span>
            <span class="author-block"><sup style="color:#ed4b82">5</sup>Harbin Institute of Technology, Shenzhen,</span><br>
            <span class="author-block"><sup style="color:#ed4b82">5</sup>The Hong Kong University of Science and Technology</span><br>
            <!--<span class="paper-block"><b style="color:#f41c1c"> ICLR 2026 </b></span>-->
          </div>
          <div class="is-size-6 publication-authors">
            <span class="author-block">* Equal contribution</span>
            <span class="author-block">&dagger; Corresponding author</span>
            <!--<span class="paper-block"><b style="color:#f41c1c">EMNLP 2024 Findings</b></span>-->
          </div>
          
          
          <div class="column has-text-centered">
            <div class="publication-links">
              <span class="link-block">
                <a href="https://openreview.net/forum?id=6CGjZYp6ft"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>OpenReview</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2510.18672" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://github.com/lqinfdim/RLMServing"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>



<section class="section">
  <div class="container" style="margin-bottom: 2vh;">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
           The reasoning large language model (RLLM) has been proven competitive in solving complex reasoning tasks such as mathematics, coding, compared to general LLM. 
            However, the serving performance and behavior of RLLM remains unexplored, which may undermine the deployment and utilization of RLLM in real-world scenario. 
            To close this gap, in this paper, we conduct a comprehensive study of RLLM service. 
            We first perform a pilot study on comparing the serving performance between RLLM and traditional LLM and reveal that there are several distinct differences regarding serving behavior: 
            (1) significant memory usage and fluctuations; 
            (2) straggler requests; 
            (3) adaptive running time; 
            (4) domain preference. 
            Then we further investigate whether existing inference optimization techniques are valid for RLLM. 
            Our main takeaways are that model quantization methods and speculative decoding can improve service system efficiency with a small compromise to RLLM accuracy, while prefix caching, 
            KV cache quantization may even degrade accuracy or serving performance for small RLLM. 
            Lastly, we conductevaluation under real world workload modeled by Gamma distribution to verify our findings. 
            Empirical results for real world workload evaluation across different dataset are aligned with our main findings regarding RLLM serving. 
            We hope our work can provide the research community and industry with insights to advance RLLM inference serving. 
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
</div>
</section>

<section class="section">
  <!-- Mechanistic Investigation. -->
  <div class="section-title-bar-wrapper">
    <div class="container has-text-centered">
      <h2 class="title is-2">Mechanistic Investigation</h2>
    </div>
  </div>
  <!--/ Mechanistic Investigation. -->
</section>

<section class="section">
  <div class="container" style="margin-top: -150px; margin-bottom: -100px;">
    <div class="columns is-centered m-6">
      <div class="column is-full has-text-centered content">
        <div class="content has-text-centered">
          <img src="figures/investigation.png" alt="mechanistic_investigation" style="max-width: 100%;"/>
          <p> 
            <span class="hermes">Visualization of the average attention weights (log scale) for user queries over video tokens in \llava with a FIFO KV cache budget of 6K video tokens per layer, averaged across 300 user video questions.<br/>
          </p> 
        </div>
      </div>
    </div>
</section>

<section class="section">
  <!-- Overview. -->
  <div class="section-title-bar-wrapper">
    <div class="container has-text-centered">
      <h2 class="title is-2">Overview</h2>
    </div>
  </div>
  <!--/ Overview. -->
</section>

<section class="section">
  <div class="container" style="margin-top: -150px; margin-bottom: -100px;">
    <div class="columns is-centered m-6">
      <div class="column is-full has-text-centered content">
        <div class="content has-text-centered">
          <img src="figures/overview.png" alt="overview" style="max-width: 100%;"/>
          <p> 
            <span class="hermes">Overview of the HERMES architecture for streaming video QA. By implementing a hierarchical KV cache and specialized management strategies, HERMES enables real-time and accurate responses through direct cache reuse, eliminating the need for additional retrieval operations or external memory whenever users pose questions.<br/>
          </p> 
        </div>
      </div>
    </div>
</section>

<section class="section">
  <!-- Experimental Results. -->
  <div class="section-title-bar-wrapper">
    <div class="container has-text-centered">
      <h2 class="title is-2">Experimental Results</h2>
    </div>
  </div>
  <!--/ Experimental Results. -->
</section>

<section class="section">
  <div class="container" style="margin-top: -100px; margin-bottom: -50px;">
    <div class="columns is-centered has-text-centered" style="margin-bottom: -30px;">
      <div class="column is-four-fifths">
        <h3 class="subsection-title">Accuracy Performance</h3>
      </div>
    </div>
    <div class="columns is-centered m-6">
      <div class="column is-full has-text-centered content">
        <div class="result-card">
          <img src="figures/results_streaming.png" alt="results" style="max-width: 100%;"/>
          <p> 
            <span class="hermes">Performance comparison (%) on StreamingBench and OVO-Bench. The "Avg." column reports the results of the average accuracy of real-time visual perception and backward tracing tasks.<br/>
          </p> 
        </div>
      </div>
    </div>
    <div class="columns is-centered m-6">
      <div class="column is-full has-text-centered content">
        <div class="result-card">
          <img src="figures/results_streaming_rvs.png" alt="results" style="max-width: 100%;"/>
          <p> 
            <span class="hermes">Performance on RVS-Ego and RVS-Movie. &dagger;: ReKV caches the KV states of all previously seen frames and is therefore treated as an upper bound.<br/>
          </p> 
        </div>
      </div>
    </div>
    <div class="columns is-centered m-6">
      <div class="column is-full has-text-centered content">
        <div class="result-card">
          <img src="figures/results_offline.png" alt="results" style="max-width: 100%;"/>
          <p> 
            <span class="hermes">Performance comparison (%) on offline benchmarks.<br/>
          </p> 
        </div>
      </div>
    </div>
</section>

<section class="section">
  <div class="container" style="margin-top: -50px; margin-bottom: -50px;">
    <div class="columns is-centered has-text-centered" style="margin-bottom: -30px;">
      <div class="column is-four-fifths">
        <h3 class="subsection-title">Efficiency Analysis</h3>
      </div>
    </div>
    <div class="columns is-centered m-6">
      <div class="column is-full has-text-centered content">
        <div class="result-card">
          <img src="figures/results_efficiency.png" alt="results" style="max-width: 100%;"/>
          <p> 
            <span class="hermes">GPU memory and TTFT latency comparison across input frame numbers. HERMES achieves 10&times; faster in TTFT compared to prior SOTA.<br/>
          </p> 
        </div>
      </div>
    </div>
    <div class="columns is-centered m-6">
      <div class="column is-full has-text-centered content">
        <div class="result-card">
          <img src="figures/results_efficiency_chunk_size.png" alt="results" style="max-width: 100%;"/>
          <p> 
            <span class="hermes">Efficiency across input frame numbers under two chunk sizes. "TTFT" denotes <i>Time to First Token</i> and "TPOT" denotes <i>Time Per Output Token</i>.<br/>
          </p> 
        </div>
      </div>
    </div>
</section>


<!-- @PAN TODO: bibtex -->
<section class="section" id="Citation">
  <div class="container is-max-desktop content">
    <h2 class="title is-3 has-text-centered">Citation</h2>
    <pre><code>
              @inproceedings{
              rllm-serving,
              title={Reasoning Language Model Inference Serving Unveiled: An Empirical Study},
              author={Li, Qi and Wu, Junpan and Liu, Xiang and Wang, Yuxin and Li, Zeyu and Tang, Zhenheng and Chen, Yuhan and Shi, Shaohuai and Chu, Xiaowen},
              booktitle={The Fourteenth International Conference on Learning Representations},
              year={2026},
              url={https://openreview.net/forum?id=6CGjZYp6ft}
              }
    </code></pre>
  </div>
</section>

<!-- <section>
  <div class="section" id="org-banners" style="display:flex">
    <a href="https://www.yale.edu/" target="_blank" rel="external">
        <img class="center-block org-banner" src="static/images/yale.png">
    </a>
    <a href="https://www.nyu.edu/" target="blank" rel="external">
        <img class="center-block org-banner" src="static/images/nyu.png">
    </a>
    <a href="https://shanghai.nyu.edu/" target="_blank" rel="external">
        <img class="center-block org-banner" src="static/images/nyush.png">
    </a>
    <a href="https://www.psu.edu/" target="_blank" rel="external">
      <img class="center-block org-banner" src="static/images/psu.png">
    </a>
    <a href="https://allenai.org/" target="_blank" rel="external">
      <img class="center-block org-banner" src="static/images/ai2.png">
    </a>
  </div>
</section> -->


<footer class="footer">
  <!-- <div class="container"> -->
    <div class="content has-text-centered">
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is adapted from <a href="https://mathvista.github.io/">MathVista</a>, licensed under <a href="https://creativecommons.org/licenses/by-sa/4.0/">Creative Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
        </div>
      </div>
    </div>
  <!-- </div> -->
</footer>

</body>
</html>
